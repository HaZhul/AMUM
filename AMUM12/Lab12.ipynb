{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97d000d",
   "metadata": {},
   "source": [
    "# Algorytmika i matematyka uczenia maszynowego \n",
    "## Laboratorium 12\n",
    "\n",
    "### Zadanie 1\n",
    "\n",
    "Zaimplementuj systemu rekomendacji filmów w oparciu o indeks Jaccarda. System ma za zadanie zwrócić listę filmów sugerowany dla podanego użytownika.\n",
    "\n",
    "Dane zostały pobrane z serwisu Kaggle z https://www.kaggle.com/datasets/gargmanas/movierecommenderdataset\n",
    "\n",
    "Zbiór zawiera dwa pliki:\n",
    "- `movies.csv` lista filmów wraz z ich identyfikatorami\n",
    "- `ratings.csv` lista ocen filmów przez użytkowników\n",
    "\n",
    "Wykonaj:\n",
    "* Wczytaj oba pliki.\n",
    "* Zamień wszystkie oceny użytkowników na wartości binarne, np. 1 jeżeli oceniony pozytywnie (zastosuj próg okreśjący np. ocena >3 oznacza pozytywną ocenę).\n",
    "* Stwórz macierz ocen użytkowników w której wiersze będą reprezentowac użytkow, a w kolumny filmy. Wartość w macierzy jest flagą mówiącą czy użytkownik ocenił film pozytywnie czy tez nie. \n",
    "* Wypełnij brakujące wartości zerami.\n",
    "* Utwórz macierz podobieństwa Jaccarda pomiędzy użytkownikami (każdy z każdym).\n",
    "    - Możesz wykorzystać funkcję [jaccard](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jaccard.html) z biblioteki scipy.\n",
    "* Zaimplementuj funkcję która dla podanego użytkownika zwróci listę sugerowanych filmów.\n",
    "    - Wybierz $n$ najbardziej podobnych użytkownikow.\n",
    "    - Sprawdz jakie filmy najczesciej wystepuja w zbiorze i zwroc je.\n",
    "    - Pamietaj: funkcja powinna zwrócić listę filmów które nie były ocenione przez użytkownika wczesniej.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44b57ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dominik\\anaconda3\\envs\\amumu\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2317: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rekomendowane filmy dla użytkownika 1: [1036, 1200, 1259, 589, 1387, 2194, 2791, 1663, 377, 2683]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "ratings['binary_rating'] = ratings['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "user_movie_matrix = ratings.pivot_table(index='userId', columns='movieId', values='binary_rating', fill_value=0)\n",
    "\n",
    "user_movie_matrix_np = user_movie_matrix.to_numpy()\n",
    "\n",
    "user_similarity = 1 - pairwise_distances(user_movie_matrix_np, metric='jaccard')\n",
    "\n",
    "\n",
    "\n",
    "def recommend_movies(user_id, user_movie_matrix, user_similarity, n_neighbors=5):\n",
    "    similar_users = user_similarity[user_id - 1].argsort()[-(n_neighbors + 1):-1]\n",
    "    similar_users_ratings = user_movie_matrix.iloc[similar_users]\n",
    "\n",
    "    user_ratings = user_movie_matrix.iloc[user_id - 1]\n",
    "\n",
    "    movie_recommendations = similar_users_ratings.sum(axis=0)\n",
    "    movie_recommendations = movie_recommendations[user_ratings == 0]\n",
    "    recommended_movies = movie_recommendations.sort_values(ascending=False).index.tolist()\n",
    "    return recommended_movies\n",
    "\n",
    "recommended_movies = recommend_movies(user_id=1, user_movie_matrix=user_movie_matrix, user_similarity=user_similarity, n_neighbors=5)\n",
    "print(\"Rekomendowane filmy dla użytkownika 1:\", recommended_movies[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15893cbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Zadanie 2\n",
    "\n",
    "\n",
    "Algorytm MinHash na przykładzie wykrywania plagiatów\n",
    "\n",
    "Wykonaj kolejno następujące kroki:\n",
    "\n",
    "1. Pobierz 8 akapitów tekstu (nie za krótkich), każdy o różnej tematyce (mogą być np. z różnych haseł Wikipedii), trzymaj się jednego języka (np. PL lub ENG). Wklej je do jednego pliku tekstowego, z linią wolną jako separatorem.\n",
    "\n",
    "2. Skopiuj wybrane 2-3 akapity i ręcznie nieco zmodyfikuj.\n",
    "\n",
    "> Przykład (z hasła https://pl.wikipedia.org/wiki/Fryderyk_Chopin):\n",
    "\n",
    "```Jest uważany za jednego z najwybitniejszych kompozytorów romantycznych, a także za jednego z najważniejszych polskich kompozytorów w historii. Był jednym z najsłynniejszych pianistów swoich czasów, często nazywany poetą fortepianu. Elementem charakterystycznym dla utworów Chopina jest pogłębiona ekspresja oraz czerpanie z wzorców stylistycznych polskiej muzyki ludowej.```\n",
    "\n",
    "↓↓↓ Zmieniono na ↓↓↓\n",
    "\n",
    "```Jest uważany za jednego z najwybitniejszych kompozytorów romantycznych, a także za jednego z najważniejszych kompozytorów polskich w historii.  Był jednym  z najsłynniejszych pianistów swoich czasów, często nazywany poetą fortepianu! Elementem charakterystycznym dla utworów Fryderyka Chopina jest pogłębiona ekspresja oraz czerpanie z wzorców polskiej muzyki ludowej.```\n",
    "\n",
    "Otrzymasz zatem w pliku tekstowym 10 lub 11 akapitów tekstu (kolejność dowolna, te \"splagiatowane\" nie muszą być na końcu).\n",
    "\n",
    "3. Z poziomu skryptu: wczytaj wszystkie akapity z pliku. Zbuduj 100 \"losowych\" funkcji haszujących.\n",
    "\n",
    "> Sugestia: funkcją \"bazową\" jest po prostu `hash(...)`. Zakładamy 64-bitową wersję Pythona 3.x, wtedy `hash(...)` jest 64-bitowy.\n",
    "Na liście seeds umieszczamy 100 losowych liczb 64-bitowych. Aby obliczyć $i$-ty hash dla ciągu \n",
    "należy wykonać `hash(s) ^ seeds[i]` (użycie operatora XOR).\n",
    "\n",
    "4. Przyjmij niewielką wartość $Q$ (np. 15), nastepnie dla każdego akapitu (zakladajac ze jego długość wynosi $n$) i dla każdej ze 100 funkcji haszujących policz hasza w przesuwnym oknie tekstu o długości $Q$ znaków (łącznie mamy $n - Q + 1$ wartości hasza); zapamiętaj MINIMUM z tych $n - Q + 1$ wartości.\n",
    "Na wyjściu mamy zatem (dla 11 akapitów) 11 * 100 wartości haszy.\n",
    "\n",
    "5. Rozważ pary akapitów \"każdy z każdym\". Jeśli dla danej pary co najmniej (np.) 30 haszy jest wspólnych, to uważamy akapity za podobne (być może plagiat) i wyświetlamy na ekranie.\n",
    "\n",
    "6. Wyświetl czas obliczeń (powinien wynosić mniej niż 0.5s).\n",
    "\n",
    "7. Poeksperymentuj z liczbą użytych funkcji haszujących, wartością, stopniem modyfikacji oryginalnych akapitów tekstu, progiem detekcji akapitów podobnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1600a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podobne akapity (indeksy):\n",
      "Akapity 4 i 6 są podobne\n",
      "Akapity 5 i 7 są podobne\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def generate_seeds(num_seeds=100):\n",
    "    random.seed(254397)\n",
    "    seeds = [random.getrandbits(64) for _ in range(num_seeds)]\n",
    "    return seeds\n",
    "\n",
    "\n",
    "def calculate_min_hashes(paragraphs, seeds, Q=15):\n",
    "    min_hashes = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        n = len(paragraph)\n",
    "        para_hashes = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            q_hashes = [\n",
    "                hash(paragraph[i:i+Q]) ^ seed for i in range(n - Q + 1)\n",
    "            ]\n",
    "            min_hash = min(q_hashes)\n",
    "            para_hashes.append(min_hash)\n",
    "\n",
    "        min_hashes.append(para_hashes)\n",
    "\n",
    "    return min_hashes\n",
    "\n",
    "def find_similar_paragraphs(min_hashes, threshold=30):\n",
    "    num_paragraphs = len(min_hashes)\n",
    "    similar_pairs = []\n",
    "\n",
    "    for i in range(num_paragraphs):\n",
    "        for j in range(i + 1, num_paragraphs):\n",
    "            common_hashes = sum(1 for a, b in zip(min_hashes[i], min_hashes[j]) if a == b)\n",
    "            if common_hashes >= threshold:\n",
    "                similar_pairs.append((i, j))\n",
    "    return similar_pairs\n",
    "\n",
    "\n",
    "with open('akapity.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "paragraphs = text.split('\\n')\n",
    "\n",
    "seeds = generate_seeds()\n",
    "Q = 15\n",
    "\n",
    "min_hashes = calculate_min_hashes(paragraphs, seeds, Q)\n",
    "similar_pairs = find_similar_paragraphs(min_hashes)\n",
    "\n",
    "print(\"Podobne akapity (indeksy):\")\n",
    "for i, j in similar_pairs:\n",
    "    print(f\"Akapity {i} i {j} są podobne\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
